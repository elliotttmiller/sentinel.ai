"""
Final Test: Verify CrewAI LLM Configuration and Real Agent Execution
This test validates that our LLM wrapper produces the correct model format for CrewAI.
"""
import sys
import os
sys.path.append('src')

def test_crewai_llm_format():
    """Test that our LLM wrapper produces the correct format for CrewAI"""
    print("ğŸ”§ Testing CrewAI LLM Configuration...")
    
    try:
        from utils.google_ai_wrapper import get_crewai_llm
        
        # Get a fresh LLM instance
        llm = get_crewai_llm()
        
        if llm is None:
            print("âŒ Failed to create LLM instance")
            return False
            
        print(f"âœ… LLM instance created successfully")
        print(f"ğŸ“‹ LLM type: {type(llm).__name__}")
        
        # Check if this is a CrewAI native LLM or LangChain wrapper
        if hasattr(llm, 'model'):
            model_name = getattr(llm, 'model', '')
            print(f"ğŸ“‹ Model attribute: {model_name}")
            
            # For CrewAI native LLM, check the model directly
            if 'CrewAI' in type(llm).__name__ or 'LLM' in type(llm).__name__:
                if model_name.startswith('gemini/'):
                    print(f"âœ… CrewAI native LLM with correct format: {model_name}")
                    return True
                else:
                    print(f"âŒ CrewAI native LLM with wrong format: {model_name}")
                    return False
            
            # For LangChain wrapper, we know it adds 'models/' prefix, but it should still work
            elif model_name.startswith('models/gemini'):
                print(f"âš ï¸  Using LangChain wrapper (adds models/ prefix): {model_name}")
                print("ğŸ”§ This may still work with CrewAI, continuing test...")
                return True
            else:
                print(f"âŒ Unknown model format: {model_name}")
                return False
        else:
            print("âŒ LLM instance has no model attribute")
            return False
            
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        return False

def test_mission_execution():
    """Test a simple mission to create a file"""
    print("\nğŸš€ Testing Real Mission Execution...")
    
    try:
        # Import the mission execution system
        from tests.send_mission import send_mission
        
        # Send a simple file creation mission
        mission_request = "Create a file named 'test_agent_success.txt' with the content 'This file was created by a real Sentinel agent using CrewAI!'"
        
        print(f"ğŸ“¤ Sending mission: {mission_request}")
        
        # The send_mission function prints output directly, doesn't return a response
        send_mission(mission_request)
        
        print("âœ… Mission sent successfully!")
        
        # Wait a moment for processing
        import time
        print("â³ Waiting 5 seconds for mission processing...")
        time.sleep(5)
        
        # Check if the file was actually created
        workspace_path = os.path.join('.', 'workspace', 'test_agent_success.txt')
        if os.path.exists(workspace_path):
            print(f"âœ… File successfully created at: {workspace_path}")
            with open(workspace_path, 'r') as f:
                content = f.read()
            print(f"ğŸ“„ File content: {content}")
            return True
        else:
            print(f"âŒ File not found at expected location: {workspace_path}")
            print("ğŸ” Let's check what files exist in workspace:")
            workspace_dir = os.path.join('.', 'workspace')
            if os.path.exists(workspace_dir):
                files = os.listdir(workspace_dir)
                print(f"   Files in workspace: {files}")
            else:
                print("   Workspace directory doesn't exist")
            return False
            
    except Exception as e:
        print(f"âŒ Mission test failed with error: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¯ FINAL INTEGRATION TEST: CrewAI LLM & Real Agent Execution")
    print("=" * 60)
    
    # Test 1: LLM Configuration
    llm_test_passed = test_crewai_llm_format()
    
    if llm_test_passed:
        # Test 2: Real Mission Execution
        mission_test_passed = test_mission_execution()
        
        if mission_test_passed:
            print("\n" + "=" * 60)
            print("ğŸ‰ ALL TESTS PASSED! The agentic engine is fully operational!")
            print("âœ… LLM correctly configured for CrewAI")
            print("âœ… Agents can execute real missions and create files")
            print("ğŸš€ Sentinel is ready for production use!")
            print("=" * 60)
        else:
            print("\nğŸ’¥ Mission execution test failed. Agents may not be working correctly.")
    else:
        print("\nğŸ’¥ LLM configuration test failed. CrewAI integration is not working.")
