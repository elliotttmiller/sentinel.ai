#!/usr/bin/env python3
"""
Simple test to verify the model name formatting logic without external dependencies.
"""

def test_model_name_mapping():
    """Test the model name mapping logic"""
    
    def custom_model_name(model):
        """Replicated model name mapping logic"""
        print(f"  Input: {model}")
        
        # Handle the specific case where models/ prefix is added by langchain
        if model.startswith("models/gemini"):
            # Extract just the model name after the last slash
            clean_name = model.split('/')[-1]
            result = f"gemini/{clean_name}"
            print(f"  Mapped models/gemini format: {model} -> {result}")
            return result
        elif model.startswith("models/"):
            # Remove models/ prefix and add gemini/ prefix
            clean_name = model.replace("models/", "")
            result = f"gemini/{clean_name}"
            print(f"  Mapped models/ format: {model} -> {result}")
            return result
        elif not model.startswith("gemini/"):
            # Add gemini/ prefix if not present
            result = f"gemini/{model}"
            print(f"  Added gemini prefix: {model} -> {result}")
            return result
        else:
            # Already in correct format
            print(f"  Model already in correct format: {model}")
            return model
    
    # Test cases based on the error message
    test_cases = [
        "models/gemini/gemini-1.5-pro",  # The problematic format from error logs
        "models/gemini-1.5-pro",
        "gemini-1.5-pro",
        "gemini/gemini-1.5-pro",
        "gemini-1.5-pro-latest",
        "models/gemini-1.5-flash",
    ]
    
    print("üß™ Testing model name mapping logic:")
    print("="*60)
    
    all_correct = True
    for test_model in test_cases:
        print(f"\nTest case: {test_model}")
        result = custom_model_name(test_model)
        
        # Verify the result is in the correct format
        if result.startswith("gemini/") and not result.startswith("models/"):
            print(f"  ‚úÖ Result: {result} (CORRECT)")
        else:
            print(f"  ‚ùå Result: {result} (INCORRECT)")
            all_correct = False
    
    print("\n" + "="*60)
    if all_correct:
        print("üéâ All model name mappings are correct!")
        print("‚úÖ The fix should resolve the 'LLM Provider NOT provided' error.")
    else:
        print("‚ùå Some model name mappings are incorrect.")
        print("‚ö†Ô∏è  The fix may need additional work.")
    
    return all_correct

def test_crewai_llm_wrapper_logic():
    """Test the CrewAI LLM wrapper logic"""
    
    print("\nüîß Testing CrewAI LLM wrapper logic:")
    print("="*60)
    
    def clean_model_name_logic(model_name):
        """Replicated logic from CrewAICompatibleLLM"""
        print(f"  Input model_name: {model_name}")
        
        # Clean the model name - ensure it's just the model name without any prefixes
        if '/' in model_name:
            clean_model_name = model_name.split('/')[-1]
            print(f"  Cleaned model_name: {clean_model_name}")
        else:
            clean_model_name = model_name
            print(f"  Model name was already clean: {clean_model_name}")
            
        # Store the correct format that litellm expects
        litellm_model_name = f"gemini/{clean_model_name}"
        print(f"  LiteLLM format: {litellm_model_name}")
        
        return clean_model_name, litellm_model_name
    
    test_models = [
        "models/gemini/gemini-1.5-pro",
        "gemini/gemini-1.5-pro", 
        "gemini-1.5-pro",
        "models/gemini-1.5-pro",
    ]
    
    all_correct = True
    for model in test_models:
        print(f"\nProcessing: {model}")
        clean, litellm_format = clean_model_name_logic(model)
        
        # Check if the litellm format is correct
        if litellm_format.startswith("gemini/") and not "models/" in litellm_format:
            print(f"  ‚úÖ LiteLLM format is correct: {litellm_format}")
        else:
            print(f"  ‚ùå LiteLLM format is incorrect: {litellm_format}")
            all_correct = False
    
    print("\n" + "="*60)
    if all_correct:
        print("üéâ CrewAI LLM wrapper logic is correct!")
    else:
        print("‚ùå CrewAI LLM wrapper logic has issues.")
    
    return all_correct

def main():
    """Run the tests"""
    print("üöÄ Running LLM Fix Logic Verification (No Dependencies)")
    print("="*70)
    
    test1_result = test_model_name_mapping()
    test2_result = test_crewai_llm_wrapper_logic()
    
    print("\n" + "="*70)
    print("FINAL SUMMARY")
    print("="*70)
    
    if test1_result and test2_result:
        print("üéâ ALL TESTS PASSED!")
        print("‚úÖ The LLM fix logic should resolve the model name format issue.")
        print("‚úÖ Expected to fix: 'litellm.BadRequestError: LLM Provider NOT provided'")
        return 0
    else:
        print("‚ùå SOME TESTS FAILED!")
        print("‚ö†Ô∏è  The fix logic may need additional refinement.")
        return 1

if __name__ == "__main__":
    exit(main())